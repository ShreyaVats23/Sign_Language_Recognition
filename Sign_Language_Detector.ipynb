{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNcim+NU5zXJA6lub+2pV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyaVats23/Sign_Language_Recognition/blob/main/Sign_Language_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0dANy_zFduUC"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import libraries\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Set dataset path from kagglehub\n",
        "import kagglehub\n",
        "dataset_path = kagglehub.dataset_download(\"harshvardhan21/sign-language-detection-using-images\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJJRmqYfd4rV",
        "outputId": "b602437a-cc31-4f20-e858-017739ed4e14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/harshvardhan21/sign-language-detection-using-images?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 268M/268M [00:01<00:00, 163MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/harshvardhan21/sign-language-detection-using-images/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Preprocess the data\n",
        "\n",
        "# Define paths\n",
        "train_dir = os.path.join(dataset_path, \"data\", \"train\")\n",
        "test_dir = os.path.join(dataset_path, \"data\", \"test\")\n"
      ],
      "metadata": {
        "id": "CjY9V1aLd8q_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose of ImageDataGenerator**\n",
        "\n",
        "\n",
        "ImageDataGenerator is used to preprocess and augment image data on the fly (as it’s fed to the model).\n",
        "\n",
        "This helps prevent overfitting and improves the model’s ability to generalize to new, unseen images.\n",
        "\n"
      ],
      "metadata": {
        "id": "XiYAdD0ViUaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set correct base directory\n",
        "base_dir = os.path.join(dataset_path, \"data\")\n",
        "\n",
        "# Augment only training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Data generators with augmentation for training\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(28, 28),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(28, 28),\n",
        "    color_mode='grayscale',\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koDlj-85eA1v",
        "outputId": "ff4890ef-cfb5-42ee-bed2-ff70128843e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 33600 images belonging to 35 classes.\n",
            "Found 8400 images belonging to 35 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Build the model\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28, 1)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),  # 50% neurons dropped\n",
        "    Dense(35, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "-3XiOUxye2c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f68320f6-ee9a-452a-f462-9e09c7eebf57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "fFTZ00YufbgC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train the model\n",
        "model.fit(train_generator, epochs=20, validation_data=val_generator)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFyHr0MAfdz3",
        "outputId": "704d14d8-255e-44b6-8476-1baa08ab1cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 43ms/step - accuracy: 0.3725 - loss: 2.2362 - val_accuracy: 0.8743 - val_loss: 0.5614\n",
            "Epoch 2/20\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 43ms/step - accuracy: 0.7517 - loss: 0.8176 - val_accuracy: 0.9325 - val_loss: 0.3076\n",
            "Epoch 3/20\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 41ms/step - accuracy: 0.8231 - loss: 0.5853 - val_accuracy: 0.9494 - val_loss: 0.2266\n",
            "Epoch 4/20\n",
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 42ms/step - accuracy: 0.8545 - loss: 0.4732 - val_accuracy: 0.9496 - val_loss: 0.1931\n",
            "Epoch 5/20\n",
            "\u001b[1m 724/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.8709 - loss: 0.4190"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(val_generator)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "id": "J63ZceB5ff9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map class indices to label names\n",
        "class_indices = val_generator.class_indices  # e.g., {'A': 0, 'B': 1, ...}\n",
        "index_to_label = {v: k for k, v in class_indices.items()}\n",
        "\n"
      ],
      "metadata": {
        "id": "lqpXOPC0owoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Build class index map\n",
        "class_indices = val_generator.class_indices\n",
        "index_to_label = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "# Store first seen example of each class\n",
        "seen = {}\n",
        "batch_count = len(val_generator)\n",
        "\n",
        "# Loop through all batches\n",
        "for batch_num in range(batch_count):\n",
        "    images, labels = val_generator[batch_num]\n",
        "    for i in range(len(images)):\n",
        "        label_index = np.argmax(labels[i])\n",
        "        if label_index not in seen:\n",
        "            seen[label_index] = (images[i], batch_num, i)\n",
        "        if len(seen) == len(class_indices):\n",
        "            break\n",
        "    if len(seen) == len(class_indices):\n",
        "        break\n",
        "\n",
        "# Plot one sample per class with its batch:index and label\n",
        "cols = 10\n",
        "rows = int(np.ceil(len(seen) / cols))\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(18, 3 * rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (label_index, (img, b, i)) in enumerate(sorted(seen.items())):\n",
        "    axes[idx].imshow(img.reshape(28, 28), cmap='gray')\n",
        "    axes[idx].set_title(f'Batch:{b}, Index:{i}\\nLabel: {index_to_label[label_index]}')\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "# Hide unused axes\n",
        "for j in range(idx + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"One Sample per Sign — Use Index to Choose\", fontsize=18, y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BEIYCGO8tSuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change batch_num and img_index according to the above image you want\n",
        "batch_num = 255\n",
        "img_index = 0\n",
        "\n",
        "# Extract image and true label\n",
        "img = val_generator[batch_num][0][img_index]\n",
        "true_index = np.argmax(val_generator[batch_num][1][img_index])\n",
        "\n",
        "# Predict\n",
        "pred = model.predict(np.expand_dims(img, axis=0))\n",
        "pred_index = np.argmax(pred)\n",
        "\n",
        "# Map to labels\n",
        "class_indices = val_generator.class_indices\n",
        "index_to_label = {v: k for k, v in class_indices.items()}\n",
        "pred_label = index_to_label[pred_index]\n",
        "true_label = index_to_label[true_index]\n",
        "print(f\"Predicted class: {pred_label}, Actual class: {true_label}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mXK4OToCf0px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "val_generator[0] returns a tuple of (images, labels) for batch 0\n",
        "\n",
        "val_generator[0][0] → just the images in that batch\n",
        "\n",
        "val_generator[0][1] → the labels in that batch\n",
        "\n",
        "val_generator[0][0][3] → the 4th image in the batch\n",
        "\n",
        "val_generator[0][1][3] → the 4th label"
      ],
      "metadata": {
        "id": "8LCu2wPHrDnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Visualize the sign with labels\n",
        "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
        "plt.title(f\"Predicted: {pred_label}, Actual: {true_label}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LeZ0dKOPf2jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jWUEpzC9q_Q_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}