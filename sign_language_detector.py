# -*- coding: utf-8 -*-
"""Sign_Language_Detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XYT3wCdpCu8AOVbeMImJ4imHvnfoHtb4
"""

# Step 1: Import libraries
import os
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np

# Step 2: Set dataset path from kagglehub
import kagglehub
dataset_path = kagglehub.dataset_download("harshvardhan21/sign-language-detection-using-images")

print("Path to dataset files:", dataset_path)

# Step 3: Preprocess the data

# Define paths
train_dir = os.path.join(dataset_path, "data", "train")
test_dir = os.path.join(dataset_path, "data", "test")

"""**Purpose of ImageDataGenerator**


ImageDataGenerator is used to preprocess and augment image data on the fly (as it’s fed to the model).

This helps prevent overfitting and improves the model’s ability to generalize to new, unseen images.


"""

# Set correct base directory
base_dir = os.path.join(dataset_path, "data")

# Augment only training data
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    validation_split=0.2
)

# Data generators with augmentation for training
train_generator = train_datagen.flow_from_directory(
    base_dir,
    target_size=(28, 28),
    color_mode='grayscale',
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_generator = train_datagen.flow_from_directory(
    base_dir,
    target_size=(28, 28),
    color_mode='grayscale',
    batch_size=32,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

# Step 4: Build the model
from tensorflow.keras.layers import Dropout

model = Sequential([
    Flatten(input_shape=(28, 28, 1)),
    Dense(64, activation='relu'),
    Dropout(0.5),  # 50% neurons dropped
    Dense(35, activation='softmax')
])

# Step 5: Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Step 6: Train the model
model.fit(train_generator, epochs=20, validation_data=val_generator)

# Step 7: Evaluate the model
test_loss, test_acc = model.evaluate(val_generator)
print(f'Test accuracy: {test_acc}')

# Map class indices to label names
class_indices = val_generator.class_indices  # e.g., {'A': 0, 'B': 1, ...}
index_to_label = {v: k for k, v in class_indices.items()}

import matplotlib.pyplot as plt
import numpy as np

# Build class index map
class_indices = val_generator.class_indices
index_to_label = {v: k for k, v in class_indices.items()}

# Store first seen example of each class
seen = {}
batch_count = len(val_generator)

# Loop through all batches
for batch_num in range(batch_count):
    images, labels = val_generator[batch_num]
    for i in range(len(images)):
        label_index = np.argmax(labels[i])
        if label_index not in seen:
            seen[label_index] = (images[i], batch_num, i)
        if len(seen) == len(class_indices):
            break
    if len(seen) == len(class_indices):
        break

# Plot one sample per class with its batch:index and label
cols = 10
rows = int(np.ceil(len(seen) / cols))
fig, axes = plt.subplots(rows, cols, figsize=(18, 3 * rows))
axes = axes.flatten()

for idx, (label_index, (img, b, i)) in enumerate(sorted(seen.items())):
    axes[idx].imshow(img.reshape(28, 28), cmap='gray')
    axes[idx].set_title(f'Batch:{b}, Index:{i}\nLabel: {index_to_label[label_index]}')
    axes[idx].axis('off')

# Hide unused axes
for j in range(idx + 1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.suptitle("One Sample per Sign — Use Index to Choose", fontsize=18, y=1.02)
plt.show()

# change batch_num and img_index according to the above image you want
batch_num = 255
img_index = 0

# Extract image and true label
img = val_generator[batch_num][0][img_index]
true_index = np.argmax(val_generator[batch_num][1][img_index])

# Predict
pred = model.predict(np.expand_dims(img, axis=0))
pred_index = np.argmax(pred)

# Map to labels
class_indices = val_generator.class_indices
index_to_label = {v: k for k, v in class_indices.items()}
pred_label = index_to_label[pred_index]
true_label = index_to_label[true_index]
print(f"Predicted class: {pred_label}, Actual class: {true_label}")

"""val_generator[0] returns a tuple of (images, labels) for batch 0

val_generator[0][0] → just the images in that batch

val_generator[0][1] → the labels in that batch

val_generator[0][0][3] → the 4th image in the batch

val_generator[0][1][3] → the 4th label
"""

# Step 9: Visualize the sign with labels
plt.imshow(img.reshape(28, 28), cmap='gray')
plt.title(f"Predicted: {pred_label}, Actual: {true_label}")
plt.axis('off')
plt.show()

